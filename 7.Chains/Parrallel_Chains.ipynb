{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd19ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm1 = HuggingFaceEndpoint(\n",
    "    repo_id='google/gemma-2-2b-it',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "llm2 = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "\n",
    "model1 = ChatHuggingFace(llm=llm1)\n",
    "model2 = ChatHuggingFace(llm=llm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48158b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template='Give me notes in simple words with real world examples on the following text \\n {text1}',\n",
    "    input_variables=['text1']\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template='Generate 5 question answers from the following text \\n {text2}',\n",
    "    input_variables=['text2']\n",
    ")\n",
    "\n",
    "prompt3=PromptTemplate(\n",
    "    template='Merge the provided notes and quiz to a single document \\n notes -> {notes} and Quiz -> {quiz}',\n",
    "    input_variables=['notes','quiz']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6051c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2086bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrallel_chain=RunnableParallel({\n",
    "    'notes': prompt1 | model1 | parser,\n",
    "    'quiz': prompt2 | model2 | parser\n",
    "})\n",
    "\n",
    "merge_chain= prompt3 | model2 | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce4e7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= parrallel_chain | merge_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43478c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Recurrent Neural Networks (RNNs) – Detailed Notes\n",
    "1. Introduction\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. Unlike feedforward neural networks, RNNs have connections that form directed cycles, allowing information to persist and enabling them to maintain a form of memory of previous inputs. This makes them particularly suitable for tasks where context and order matter, such as natural language processing, time series forecasting, and speech recognition.\n",
    "\n",
    "2. Why RNNs?\n",
    "Traditional neural networks do not have an internal memory and treat each input independently. This is inefficient for sequential tasks where the output depends on the previous computations or inputs. RNNs address this limitation by:\n",
    "\n",
    "Processing inputs one step at a time (sequentially).\n",
    "\n",
    "Maintaining a hidden state that gets updated with each input.\n",
    "\n",
    "Sharing parameters across all time steps.\n",
    "\n",
    "This allows RNNs to learn patterns over time and make predictions based on both current input and prior context.\n",
    "\n",
    "3. Architecture of RNN\n",
    "At each time step t, an RNN receives:\n",
    "\n",
    "An input vector xₜ\n",
    "\n",
    "The previous hidden state hₜ₋₁\n",
    "\n",
    "And computes:\n",
    "\n",
    "The current hidden state hₜ using the formula:\n",
    "hₜ = tanh(Wₕₕ·hₜ₋₁ + Wₓₕ·xₜ + bₕ)\n",
    "\n",
    "An optional output yₜ, often computed as:\n",
    "yₜ = Wₕy·hₜ + b_y\n",
    "\n",
    "Where:\n",
    "\n",
    "Wₓₕ is the weight matrix for input to hidden state\n",
    "\n",
    "Wₕₕ is the weight matrix for hidden to hidden state\n",
    "\n",
    "Wₕy is the weight matrix for hidden to output\n",
    "\n",
    "bₕ and b_y are biases\n",
    "\n",
    "All time steps share these weights, which significantly reduces the number of parameters.\n",
    "\n",
    "4. Sequence Modeling with RNNs\n",
    "RNNs can be configured in different ways depending on the task:\n",
    "\n",
    "One-to-One: A single input produces a single output (e.g., image classification).\n",
    "\n",
    "One-to-Many: A single input produces a sequence of outputs (e.g., image captioning).\n",
    "\n",
    "Many-to-One: A sequence of inputs produces a single output (e.g., sentiment analysis).\n",
    "\n",
    "Many-to-Many: A sequence of inputs produces a sequence of outputs (e.g., machine translation).\n",
    "\n",
    "5. Backpropagation Through Time (BPTT)\n",
    "Training an RNN involves unfolding it in time and applying backpropagation, a process known as Backpropagation Through Time (BPTT). In BPTT:\n",
    "\n",
    "The loss is computed at each time step.\n",
    "\n",
    "Gradients are propagated backward through each time step.\n",
    "\n",
    "Weight updates are made by accumulating gradients over time.\n",
    "\n",
    "This allows RNNs to learn long-term dependencies, but also introduces challenges due to the recursive nature of the network.\n",
    "\n",
    "6. Challenges with RNNs\n",
    "Despite their strengths, RNNs have several limitations:\n",
    "\n",
    "a. Vanishing Gradient Problem\n",
    "During BPTT, gradients can become very small as they are multiplied across many time steps. This leads to extremely slow learning or an inability to learn long-range dependencies.\n",
    "\n",
    "b. Exploding Gradient Problem\n",
    "Gradients can also grow exponentially, causing instability in training.\n",
    "\n",
    "c. Short-Term Memory\n",
    "Due to vanishing gradients, RNNs tend to remember only short-term dependencies unless special mechanisms are introduced.\n",
    "\n",
    "7. Variants of RNNs\n",
    "To address the limitations of vanilla RNNs, several improved architectures were developed:\n",
    "\n",
    "a. Long Short-Term Memory (LSTM)\n",
    "LSTM introduces memory cells and gates (input, forget, output gates) that control the flow of information, allowing the network to remember or forget information over longer periods.\n",
    "\n",
    "b. Gated Recurrent Unit (GRU)\n",
    "GRUs simplify LSTM by combining the forget and input gates into a single update gate, reducing complexity while retaining the ability to learn long-term dependencies.\n",
    "\n",
    "c. Bidirectional RNN\n",
    "These RNNs process data in both forward and backward directions, which helps in capturing context from both past and future.\n",
    "\n",
    "8. Applications of RNNs\n",
    "RNNs are widely used in domains where sequential information is crucial:\n",
    "\n",
    "Natural Language Processing (NLP): Sentiment analysis, machine translation, text generation, question answering.\n",
    "\n",
    "Speech Recognition: Transcribing audio signals into text.\n",
    "\n",
    "Time Series Prediction: Forecasting stock prices, weather, or sales.\n",
    "\n",
    "Music and Audio: Music composition and audio generation.\n",
    "\n",
    "Video Processing: Action recognition and captioning.\n",
    "\n",
    "9. Limitations in Modern Context\n",
    "While RNNs laid the foundation for sequential modeling, they are increasingly being replaced or complemented by models like Transformers, which handle sequences using self-attention mechanisms rather than recurrence. Transformers offer better parallelization, handle long-range dependencies more effectively, and have become state-of-the-art in many NLP tasks.\n",
    "\n",
    "10. Conclusion\n",
    "Recurrent Neural Networks are powerful tools for modeling sequential data. Their ability to maintain hidden states and learn patterns over time makes them suitable for a variety of temporal and linguistic tasks. However, due to their training difficulties and limitations with long-term dependencies, they are often augmented or replaced by advanced architectures like LSTM, GRU, or Transformers in modern applications.\n",
    "\n",
    "Understanding RNNs is fundamental to grasping the evolution of deep learning models for sequence data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c50c910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"\"\"\n",
    "Convolutional Neural Networks (CNNs) – Detailed Theoretical Notes\n",
    "1. Introduction\n",
    "Convolutional Neural Networks (CNNs) are a specialized type of artificial neural networks designed primarily for processing structured grid data, such as images. Unlike traditional feedforward neural networks, CNNs are particularly efficient in handling spatial hierarchies in images through the use of convolutional operations.\n",
    "\n",
    "CNNs were inspired by the visual cortex of animals, where individual neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.\n",
    "\n",
    "2. Motivation Behind CNNs\n",
    "In classical machine learning and fully connected networks, each neuron connects to every input pixel, which becomes computationally infeasible for high-dimensional data like images. Additionally, such models ignore the spatial structure of the data. CNNs overcome these limitations by:\n",
    "\n",
    "Leveraging local connections\n",
    "\n",
    "Applying weight sharing\n",
    "\n",
    "Using hierarchical feature extraction\n",
    "\n",
    "These characteristics reduce the number of parameters and preserve spatial relationships.\n",
    "\n",
    "3. Key Components of a CNN\n",
    "a. Input Layer\n",
    "The input to a CNN is typically a multi-dimensional array (e.g., a 2D grayscale image or a 3D color image). Each image has:\n",
    "\n",
    "Width (W)\n",
    "\n",
    "Height (H)\n",
    "\n",
    "Depth or Channels (C), such as 3 for RGB images\n",
    "\n",
    "b. Convolutional Layer\n",
    "This layer is the core building block of a CNN. It involves:\n",
    "\n",
    "A set of learnable filters or kernels (e.g., 3×3 or 5×5) that slide over the input image.\n",
    "\n",
    "At each location, the filter performs an element-wise multiplication followed by a summation, producing a feature map.\n",
    "\n",
    "Important concepts:\n",
    "\n",
    "Stride: The step size with which the filter moves.\n",
    "\n",
    "Padding: Adding borders to the input so that the output feature map retains dimensionality. There are two common types: \"valid\" (no padding) and \"same\" (output size equals input size).\n",
    "\n",
    "Receptive field: The region in the input that a filter looks at.\n",
    "\n",
    "The goal of convolution is to extract features such as edges, textures, shapes, etc.\n",
    "\n",
    "c. Activation Function\n",
    "After each convolution operation, a non-linear activation function (most commonly ReLU: Rectified Linear Unit) is applied to introduce non-linearity and allow the network to learn complex patterns.\n",
    "\n",
    "Formula for ReLU:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Other activation functions include Leaky ReLU, Tanh, and Sigmoid, though ReLU is most common due to its simplicity and performance.\n",
    "\n",
    "d. Pooling Layer (Subsampling or Downsampling)\n",
    "Pooling layers reduce the spatial size of feature maps, which helps:\n",
    "\n",
    "Decrease computation\n",
    "\n",
    "Reduce overfitting\n",
    "\n",
    "Introduce translation invariance\n",
    "\n",
    "Types of pooling:\n",
    "\n",
    "Max Pooling: Takes the maximum value in each region (e.g., 2×2 window)\n",
    "\n",
    "Average Pooling: Takes the average value\n",
    "\n",
    "Max pooling is more commonly used because it highlights the most prominent features.\n",
    "\n",
    "e. Fully Connected Layer (Dense Layer)\n",
    "After several convolutional and pooling layers, the output is flattened and passed to one or more fully connected layers. These layers act as a classifier and produce the final output, such as class scores in classification tasks.\n",
    "\n",
    "f. Output Layer\n",
    "For classification tasks, the final layer often uses:\n",
    "\n",
    "Softmax for multi-class classification\n",
    "\n",
    "Sigmoid for binary classification\n",
    "\n",
    "4. CNN Architecture Summary\n",
    "A typical CNN architecture looks like this:\n",
    "\n",
    "mathematica\n",
    "Copy\n",
    "Edit\n",
    "Input → [Conv → Activation → Pool]*N → Flatten → Fully Connected → Output\n",
    "Here, [Conv → Activation → Pool]*N represents N repetitions of convolution, activation, and pooling.\n",
    "\n",
    "5. Parameter Sharing and Sparsity\n",
    "CNNs are highly efficient due to:\n",
    "\n",
    "Parameter Sharing: The same filter (set of weights) is used across the entire input image, greatly reducing the number of parameters compared to fully connected layers.\n",
    "\n",
    "Sparse Connectivity: Each neuron in the convolutional layer is connected to only a small region of the input.\n",
    "\n",
    "This makes CNNs scalable and trainable on large image datasets.\n",
    "\n",
    "6. Advantages of CNNs\n",
    "Capture spatial and temporal dependencies\n",
    "\n",
    "Fewer parameters compared to fully connected networks\n",
    "\n",
    "Require minimal preprocessing\n",
    "\n",
    "Perform well in image-related tasks\n",
    "\n",
    "Translation invariance due to pooling\n",
    "\n",
    "7. Training a CNN\n",
    "CNNs are trained using the same approach as other neural networks:\n",
    "\n",
    "Forward Propagation: Input is passed through the layers to compute output.\n",
    "\n",
    "Loss Calculation: Using functions like Cross-Entropy for classification.\n",
    "\n",
    "Backpropagation: Compute gradients of loss with respect to weights.\n",
    "\n",
    "Gradient Descent Optimization: Update weights using optimizers like SGD or Adam.\n",
    "\n",
    "Techniques such as data augmentation, dropout, and batch normalization are often used to improve performance and reduce overfitting.\n",
    "\n",
    "8. Applications of CNNs\n",
    "CNNs have revolutionized computer vision and are widely used in:\n",
    "\n",
    "Image classification (e.g., recognizing objects in images)\n",
    "\n",
    "Object detection (e.g., identifying multiple objects with bounding boxes)\n",
    "\n",
    "Facial recognition\n",
    "\n",
    "Medical image analysis (e.g., tumor detection in MRIs or X-rays)\n",
    "\n",
    "Self-driving cars (e.g., lane detection, pedestrian recognition)\n",
    "\n",
    "Image segmentation\n",
    "\n",
    "Style transfer and image generation\n",
    "\n",
    "9. Variants and Enhancements\n",
    "Several advanced CNN architectures have been proposed to improve performance:\n",
    "\n",
    "LeNet: One of the earliest CNNs, used for digit recognition.\n",
    "\n",
    "AlexNet: Popularized CNNs after winning ImageNet in 2012.\n",
    "\n",
    "VGGNet: Used very deep networks with 3x3 convolutions.\n",
    "\n",
    "GoogLeNet/Inception: Used parallel convolutional layers (Inception modules).\n",
    "\n",
    "ResNet: Introduced skip connections (residual learning) to combat vanishing gradients and allow deeper networks.\n",
    "\n",
    "These architectures differ in design but all follow the core principles of CNNs.\n",
    "\n",
    "10. Limitations of CNNs\n",
    "Despite their success, CNNs have certain limitations:\n",
    "\n",
    "Lack of ability to model global relationships explicitly\n",
    "\n",
    "Depend heavily on labeled data for supervised training\n",
    "\n",
    "Require large amounts of computational power and memory\n",
    "\n",
    "Not ideal for sequential data (where RNNs or Transformers are better suited)\n",
    "\n",
    "Sensitive to adversarial attacks in security-critical applications\n",
    "\n",
    "11. Recent Trends\n",
    "Modern deep learning has seen CNNs being combined with or replaced by newer models:\n",
    "\n",
    "Vision Transformers (ViTs): These use self-attention instead of convolutions to model images.\n",
    "\n",
    "Hybrid Models: Combine CNNs with transformers or recurrent layers.\n",
    "\n",
    "Self-supervised learning: Uses unlabelled data to pretrain CNNs before fine-tuning.\n",
    "\n",
    "While CNNs remain foundational, they are now part of a broader ecosystem of computer vision models.\n",
    "\n",
    "12. Conclusion\n",
    "Convolutional Neural Networks are powerful tools for processing visual and spatial data. Their architecture, inspired by biological systems, efficiently captures local features while reducing computational complexity. From simple pattern recognition to complex object detection and scene understanding, CNNs are central to many modern AI applications. Understanding CNN theory is critical for working in fields like computer vision, medical imaging, and even audio and signal processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9b4b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'text1':text1,'text2':text2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9cc4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Recurrent Neural Networks (RNNs): Simplified Notes and Quiz\n",
      "\n",
      "### What are RNNs?\n",
      "\n",
      "Imagine a computer that remembers what happened before in a story. That's like how RNNs work! They're designed to understand sequential data, which means things happening in order. Think about sentences, music, or even stock market movements.\n",
      "\n",
      "### Why RNNs?\n",
      "\n",
      "Traditional algorithms don't care about the order things happen. They work like this:\n",
      "* **Sequence problem:** A movie doesn't make sense out of context. Imagine trying to watch it without knowing the previous details!\n",
      "* **RNN answer:** They have \"memory\", storing how past things impact the present. This lets them understand the context of words/sounds/trends.\n",
      "\n",
      "### How do they work? \n",
      "\n",
      "1. **Taking in input:** They treat each part of the sequence as a \"step\" and get initial information.\n",
      "2. **Remembering past:** The model remembers the current input, but also how it's connected to previous information (\"molecular chart\"). \n",
      "3. **Generating output:** Think of it like writing a response in response to past context or choosing the next \"step\" based on what has happened before. \n",
      "\n",
      "### Building and Scaling: \n",
      "\n",
      "These components allow RNNs to process information chronologically, learn long-term dependencies, and create more accurate outputs. \n",
      "\n",
      "### Using RNNs: \n",
      "\n",
      "Think of these examples:\n",
      "* **Language like we understand it:** Use RNNs to translate languages, summarize text, or even write personalized stories. \n",
      "* **Time-related things:** They're great for stock analysis, predicting weather, and converting speech to text.\n",
      "\n",
      "### The Challenges: \n",
      "\n",
      "RNNs are quite powerful but come with some limitations:\n",
      "* **Long-term memory:** RNNs struggle lasting over a long period, forget information too soon, like trying to remember the entire plot of a book after reading a single chapter. \n",
      "* **Vanishing gradients:** Imagine trying to calculate the distance between two points in a long, circular road.  These gradients become small, making the core task difficult, making RNNs slow for long sequences.\n",
      "\n",
      "### Advanced RNNs: \n",
      "\n",
      "To concentrate on past context, some newer versions of RNNs have features like \"memory\" and \"gates\": \n",
      "\n",
      "1. **LSTM (Long Short-Term Memory):** This is like having a  memory enhancer for RNNs.\n",
      "2. **GRU (Gated Recurrent Unit):** A simpler, yet efficient alternative to LSTM. And even lighter than. \n",
      "3. **Bidirectional RNNs:**  They process information in both forward and backward directions. This allows them to better contextualize text as it captures both previous and future sentences when reading.\n",
      "\n",
      "### Modern RNNs in the Wild: \n",
      "\n",
      "Smaller datasets are being substituted for more powerful RNN successors like Transformers because of faster processing, improved memory, and better long-range accuracy for tackling complex tasks.\n",
      "\n",
      "### In a nutshell: \n",
      "\n",
      "RNNs are a great tool, but over time the introduction of newer and more efficient techniques have brought about better results. \n",
      "\n",
      "---\n",
      "\n",
      "### RNNs Quiz\n",
      "\n",
      "1. Question: What inspired the development of Convolutional Neural Networks (CNNs)?\n",
      "Answer: CNNs were inspired by the visual cortex of animals, where individual neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.\n",
      "\n",
      "2. Question: What are the key characteristics of CNNs that make them efficient for handling spatial hierarchies in images?\n",
      "Answer: CNNs are efficient due to parameter sharing, where the same filter (set of weights) is used across the entire input image, and sparse connectivity, where each neuron is connected to only a small region of the input.\n",
      "\n",
      "3. Question: What is the purpose of the receptive field in a CNN?\n",
      "Answer: The receptive field is the region in the input that a filter looks at, and its purpose is to extract features such as edges, textures, shapes, etc.\n",
      "\n",
      "4. Question: What is the main advantage of using a ReLU activation function in a CNN?\n",
      "Answer: The main advantage of using a ReLU activation function is that it introduces non-linearity and allows the network to learn complex patterns, making it a simple and effective choice due to its simplicity and performance.\n",
      "\n",
      "5. Question: What is the main limitation of CNNs in terms of their ability to model global relationships explicitly?\n",
      "Answer: One of the main limitations of CNNs is that they lack the ability to model global relationships explicitly, which can be a limitation in certain tasks and applications.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6b0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff4449c",
   "metadata": {},
   "source": [
    "# We can use same notes in both models to create notes and quiz i have used different just to learn how it is done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc89df2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
